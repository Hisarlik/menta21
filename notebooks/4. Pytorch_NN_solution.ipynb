{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9cMjLN34v-5",
    "outputId": "adf0de7d-ed31-429f-ba46-d1dab8b1a0b8"
   },
   "outputs": [],
   "source": [
    "\n",
    "#!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJDzt4YOtMy-"
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZDb7klowQI-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh1wPZ52rwRb"
   },
   "outputs": [],
   "source": [
    "# Deterministic\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**20 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**20 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**20 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**20 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsmCoS9Ys_9F"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEInlYKG5H_o",
    "outputId": "4236a0b7-4333-4ef6-8fd4-0de88d749bfc"
   },
   "outputs": [],
   "source": [
    "# Wandb Login\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqdLjwGDunBb"
   },
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs =10,\n",
    "    batch_size = 128,\n",
    "    learning_rate = 0.001,\n",
    "    dataset = \"Authorship 2000\",\n",
    "    architecture = \"Dense:  Input, Layer 512, relu, batchnorm 512 , Layer 64, relu, batchnorm 64, dropout 0.1, output\", \n",
    "    criterion = \"BCEWithLogitsLoss\",\n",
    "    optimizer = \"Adam\",\n",
    "    path =  \"../data/large/\",\n",
    "    conf_model = joblib.load( \"../data/large/conf.pkl\")\n",
    "\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaLqbX-3j_r-"
   },
   "outputs": [],
   "source": [
    "class AuthorshipDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X_ngrams_data, X_punct_data, X_struct_data, y_data):\n",
    "    self.X_ngrams_data = X_ngrams_data\n",
    "    self.X_punct_data = X_punct_data\n",
    "    self.X_struct_data = X_struct_data\n",
    "    self.y_data = y_data\n",
    "\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return self.X_ngrams_data[index], self.X_punct_data[index], self.X_struct_data[index], self.y_data[index]\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y_data)\n",
    "\n",
    "  def vector_size(self):\n",
    "    return self.X_ngrams_data.shape[1], self.X_punct_data.shape[1], self.X_struct_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FO5tp3ooBS8"
   },
   "outputs": [],
   "source": [
    "class AuthorshipClassificationPunct(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size):\n",
    "    print(\"AuthorshipClassificationPunct\")\n",
    "    super(AuthorshipClassificationPunct, self).__init__()\n",
    " \n",
    "    print(\"input_size_punct:\",input_size[1])\n",
    "    self.layer1_punct = nn.Linear(input_size[1], 16)\n",
    "    self.layer2_punct = nn.Linear(16, 8)\n",
    "    self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "    self.dropout = nn.Dropout(p=0.1)\n",
    "    self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "    self.batchnorm2 = nn.BatchNorm1d(8)\n",
    "\n",
    "  \n",
    "  def forward(self, inputs_ngrams, inputs_punct):\n",
    "\n",
    "    x_punct = self.layer1_punct(inputs_punct)\n",
    "    x_punct = self.relu(x_punct)\n",
    "    x_punct = self.batchnorm1(x_punct)\n",
    "    x_punct = self.layer2_punct(x_punct)\n",
    "    x_punct = self.relu(x_punct)\n",
    "    x_punct = self.batchnorm2(x_punct)\n",
    "    output = self.output_layer(x_punct)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUwaH3HuloJh"
   },
   "outputs": [],
   "source": [
    "class AuthorshipClassificationNGrams(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size):\n",
    "    print(\"AuthorshipClassificationNGrams\")\n",
    "    super(AuthorshipClassificationNGrams, self).__init__()\n",
    "\n",
    "    print(\"input_size_ngrams:\",input_size[0])\n",
    "    self.layer1_ngrams = nn.Linear(input_size[0], 16)\n",
    "    #self.layer2_ngrams = nn.Linear(16, 6)\n",
    "    self.output_layer = nn.Linear(16, 1)\n",
    "\n",
    "    self.selu = nn.SELU()\n",
    "    self.dropout = nn.Dropout(p=0.5)\n",
    "    self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "\n",
    "  \n",
    "  def forward(self, inputs_ngrams, inputs_punct):\n",
    "\n",
    "    x_grams = self.layer1_ngrams(inputs_ngrams)\n",
    "    x_grams = self.selu(x_grams)\n",
    "    x_grams = self.batchnorm1(x_grams)\n",
    "    #x_grams = self.dropout(x_grams)\n",
    "    #x_grams = self.layer2_ngrams(x_grams)\n",
    "    #x_grams = self.selu(x_grams) \n",
    "    output = self.output_layer(x_grams)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ds69uUqJkf6l"
   },
   "outputs": [],
   "source": [
    "class AuthorshipClassification(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size):\n",
    "    print(\"AuthorshipClassification\")\n",
    "    super(AuthorshipClassification, self).__init__()\n",
    "\n",
    "    print(\"input_size_ngrams:\",input_size[0])\n",
    "    self.layer1_ngrams = nn.Linear(input_size[0], 16)\n",
    "    self.layer2_ngrams = nn.Linear(16, 6)\n",
    "    #self.layer3_ngrams = nn.Linear(64, 32)\n",
    "    #self.layer4_ngrams = nn.Linear(32, 16)\n",
    "    #self.layer5_ngrams = nn.Linear(16, 6)\n",
    " \n",
    "    print(\"input_size_punct:\",input_size[1])\n",
    "    self.layer1_punct = nn.Linear(input_size[1], 6)\n",
    "    #self.layer2_punct = nn.Linear(16, 6)\n",
    "    #self.layer3_punct = nn.Linear(4, 2)\n",
    "    \n",
    "    print(\"input_size_struct:\",input_size[2])\n",
    "    self.layer1_struct = nn.Linear(input_size[2], 4)\n",
    "    #self.layer2_punct = nn.Linear(16, 6)\n",
    "    #self.layer3_punct = nn.Linear(4, 2)    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    self.layer1_join = nn.Linear(16, 4)\n",
    "    self.layer2_join = nn.Linear(4, 2)\n",
    "    self.output_layer = nn.Linear(2, 1)\n",
    "\n",
    "    self.selu = nn.SELU()\n",
    "    self.dropout = nn.Dropout(p=0.5)\n",
    "    self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "    #self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "    #self.batchnorm3 = nn.BatchNorm1d(32)\n",
    "    #self.batchnorm4 = nn.BatchNorm1d(16)\n",
    "\n",
    "  \n",
    "  def forward(self, inputs_ngrams, inputs_punct, inputs_struct):\n",
    "\n",
    "    x_grams = self.layer1_ngrams(inputs_ngrams)\n",
    "    x_grams = self.selu(x_grams)\n",
    "    x_grams = self.batchnorm1(x_grams)\n",
    "    #x_grams = self.dropout(x_grams)\n",
    "    x_grams = self.layer2_ngrams(x_grams)\n",
    "    x_grams = self.selu(x_grams)\n",
    "    #x_grams = self.batchnorm2(x_grams)\n",
    "    #x_grams = self.dropout(x_grams)\n",
    "    #x_grams = self.layer3_ngrams(x_grams)\n",
    "    #x_grams = self.selu(x_grams)\n",
    "    #x_grams = self.batchnorm3(x_grams)\n",
    "    #x_grams = self.dropout(x_grams)\n",
    "    #x_grams = self.layer4_ngrams(x_grams)\n",
    "    #x_grams = self.selu(x_grams)  \n",
    "    #x_grams = self.batchnorm4(x_grams) \n",
    "    #x_grams = self.dropout(x_grams)  \n",
    "    #x_grams = self.layer5_ngrams(x_grams)\n",
    "    #x_grams = self.selu(x_grams)\n",
    "\n",
    "    x_punct = self.layer1_punct(inputs_punct)\n",
    "    x_punct = self.selu(x_punct)\n",
    "    #x_punct = self.layer2_punct(x_punct)\n",
    "    #x_punct = self.selu(x_punct)\n",
    "    #x_punct = self.relu(x_punct)\n",
    "    #x_punct = self.layer3_punct(x_punct)\n",
    "    \n",
    "    X_struct = self.layer1_struct(inputs_struct)\n",
    "    X_struct = self.selu(X_struct)\n",
    "\n",
    "\n",
    "    x = torch.cat((x_grams, x_punct, X_struct), dim=1)\n",
    "\n",
    "    x = self.layer1_join(x)\n",
    "    x = self.selu(x)\n",
    "    x = self.layer2_join(x)\n",
    "    x = self.selu(x)\n",
    "    output = self.output_layer(x)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51tXSoA4iMi2"
   },
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "\n",
    "  with wandb.init(project=\"authorship\", config=hyperparameters):\n",
    "\n",
    "    config = wandb.config\n",
    "    print(\"Calling make\")\n",
    "    model, train_loader, test_loader, criterion, optimizer = make(config)\n",
    "    print(model)\n",
    "\n",
    "    print(\"Calling train\")\n",
    "    train(model, train_loader, criterion, optimizer, config)\n",
    "\n",
    "    print(\"Calling dev\")\n",
    "    return test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(conf_global, type_data=\"train\"):\n",
    "    \n",
    "  path = conf_global.path\n",
    "  conf_model = conf_global.conf_model\n",
    "    \n",
    "    \n",
    "  print(conf_model)\n",
    "\n",
    "\n",
    "\n",
    "  if type_data == \"train\":\n",
    "    X_train_ngrams  = np.memmap(path + 'features_ngrams_X_train.npy', dtype='float32', mode='r', shape=(conf_model['rows_train'], conf_model['ngrams']))\n",
    "    X_train_punct = np.memmap(path + 'features_punct_X_train.npy', dtype='float32', mode='r', shape=(conf_model['rows_train'], conf_model['punct']))\n",
    "    X_train_struct = np.memmap(path + 'features_struct_X_train.npy', dtype='float32', mode='r', shape=(conf_model['rows_train'],4))  \n",
    "    Y_train = np.memmap(path + 'Y_train.npy', dtype='int32', mode='r', shape=(conf_model['rows_train']))\n",
    "    \n",
    "    return AuthorshipDataset(torch.from_numpy(X_train_ngrams), \n",
    "                             torch.from_numpy(X_train_punct),\n",
    "                             torch.from_numpy(X_train_struct),                            \n",
    "                             torch.from_numpy(Y_train.astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPX9bQ5SjvgQ"
   },
   "outputs": [],
   "source": [
    "def make(config):\n",
    "\n",
    "  # get_data\n",
    "  data_train = get_data(config, type_data=\"train\")\n",
    "  data_dev = get_data(config, type_data=\"train\")\n",
    "\n",
    "  data_input_size = data_train.vector_size()\n",
    "  \n",
    "  # data_loaders\n",
    "  train_loader = DataLoader(dataset=data_train, batch_size=config.batch_size, shuffle=False)\n",
    "  dev_loader = DataLoader(dataset=data_dev, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "  \n",
    "  #model\n",
    "  model = AuthorshipClassification(data_input_size).to(device)\n",
    "\n",
    "  # criterion and optimizer\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "\n",
    "  return model, train_loader, dev_loader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4psUi1aws0Nk"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(y_pred, y_test):\n",
    "\n",
    "  y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "  correct_results = (y_pred_tag == y_test).sum().float()\n",
    "  acc = correct_results / y_test.shape[0]\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opRyUUWer0HC"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, config):\n",
    "\n",
    "  # Tell wandb to watch \n",
    "  wandb.watch(model, criterion, log_freq=10)\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(1, config.epochs+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_ngrams_batch, X_punct_batch, X_struct_batch, y_batch in train_loader:\n",
    "      X_ngrams_batch, X_punct_batch, X_struct_batch, y_batch = (X_ngrams_batch.to(device), \n",
    "                                                                X_punct_batch.to(device), \n",
    "                                                                X_struct_batch.to(device),\n",
    "                                                                y_batch.to(device))\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      y_pred = model(X_ngrams_batch, X_punct_batch, X_struct_batch)\n",
    "  \n",
    "\n",
    "      loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "      acc = binary_accuracy(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "    print(f'Epoch {epoch}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')  \n",
    "    wandb.log({\n",
    "          \"Epoch\": epoch,\n",
    "          \"Train Accuracy\": epoch_acc/len(train_loader),\n",
    "          \"Train Loss\": epoch_loss/len(train_loader)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9chbla1dPjwd"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for X_ngrams_batch, X_punct_batch, X_struct_batch, y_batch in test_loader:\n",
    "            X_ngrams_batch, X_punct_batch, X_struct_batch, y_batch = (X_ngrams_batch.to(device), \n",
    "                                                    X_punct_batch.to(device), \n",
    "                                                    X_struct_batch.to(device),\n",
    "                                                    y_batch.to(device))\n",
    "            outputs = model(X_ngrams_batch, X_punct_batch, X_struct_batch)\n",
    "            y_test_pred = torch.sigmoid(outputs)\n",
    "           \n",
    "            predicted = torch.round(y_test_pred).squeeze()\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            print(f\"Accuracy of the model on the {total} \" +\n",
    "              f\"test data: {100 * correct / total}%\")\n",
    "\n",
    "\n",
    "            y_pred_tag = torch.round(y_test_pred)\n",
    "            y_pred_list.extend(y_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        wandb.log({\"test_accuracy\": correct / total})\n",
    "\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "    torch.onnx.export(model,(X_ngrams_batch, X_punct_batch, X_struct_batch),\"model.onnx\")\n",
    "    #wandb.save(\"model.onnx\")\n",
    "    return y_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvGPrqSlwpUr",
    "outputId": "139a6f2f-919f-4424-ca36-0461b46bb868"
   },
   "outputs": [],
   "source": [
    "print(\"Start experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRkntfec6WQW"
   },
   "outputs": [],
   "source": [
    "conf_model = joblib.load(\"../data/large/conf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMHzw7G47keW"
   },
   "outputs": [],
   "source": [
    "conf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "id": "9OCcuyUCvF0-",
    "outputId": "5dc13987-2779-4c86-ff2d-994af7c26216"
   },
   "outputs": [],
   "source": [
    "y_pred_list = model_pipeline(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_DATA_DIR = 'data/small/final/'\n",
    "Y_test = np.memmap(TEMP_DATA_DIR + 'Y_dev.npy', dtype='int32', mode='r', shape=(7891))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Suyaqaz-2gkH"
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qNF2t_Z25DQ"
   },
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGfjjsfT3CiE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pytorch NN solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
